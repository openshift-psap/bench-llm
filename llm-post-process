#!/usr/bin/env python3
# -*- mode: python; indent-tabs-mode: nil; python-indent-level: 4 -*-
# vim: autoindent tabstop=4 shiftwidth=4 expandtab softtabstop=4 filetype=python

import sys
import os
import lzma
import re
import copy
import math
import json
import yaml
import argparse
import glob
from datetime import datetime
from pathlib import Path

TOOLBOX_HOME = os.environ.get('TOOLBOX_HOME')
if TOOLBOX_HOME is None:
    print("This script requires libraries that are provided by the toolbox project.")
    print("Toolbox can be acquired from https://github.com/perftool-incubator/toolbox and")
    print("then use 'export TOOLBOX_HOME=/path/to/toolbox' so that it can be located.")
    exit(1)
else:
    p = Path(TOOLBOX_HOME) / 'python'
    if not p.exists() or not p.is_dir():
        print("ERROR: <TOOLBOX_HOME>/python ('%s') does not exist!" % (p))
        exit(2)
    sys.path.append(str(p))
from toolbox.metrics import log_sample
from toolbox.metrics import finish_samples

params = {}

class t_global(object):
     args = None

def process_options():
    parser = argparse.ArgumentParser(description = 'Post process raw benchmark data into Common Data Model output')

    parser.add_argument('--workflow',
                        dest = 'workflow',
                        help = 'sdg or train',
                        default = ""
                        )

    t_global.args, unknown = parser.parse_known_args()

    return()

def main():
    process_options()
    if t_global.args.workflow == '':
        print('workflow was not defined, exiting')
        return(1)

    # In any benchmark post-process script, the metrics generated need to be attributed to a
    # time-period (AKA benchmark-phase).  The period which is used to report and offical
    # result for the benchmark is the 'measurement' period.  Other periods thay may exist
    # could be "warm-up", "prep", etc.

    iter_sample = {
        'primary-period': "measurement",
        'benchmark': "llm",
        'periods': [],
        'rickshaw-bench-metric': { 'schema': { 'version': '2021.04.12' } }
        }

    metric_files = []

    if t_global.args.workflow == 'train':
        first_ts = None
        last_ts = None
        for phase in [1, 2]:
            period = { 'name': 'phase' + str(phase), 'metric-files': [] }
            file_id = 'phase' + str(phase)
            desc = {'source' : 'llm', 'class': 'throughput'}
            names = {}
            desc['type'] = 'train-samples-sec';
            filename = 'e2e/phase' + str(phase) + '/checkpoints/training_params_and_metrics_global0.jsonl.xz'
            print('Opening ' + filename)
            with lzma.open(filename, 'rt') as file:
                for line in file:
                    d = json.loads(line)
                    # file contents to parse (per line):
                    #{"epoch": 0, "step": 1, "rank": 0,
                    # "loss": 0.18146394193172455,
                    # "overall_throughput": 3.5244029279710176,
                    # "lr": 0.0, "cuda_mem_allocated": 14.08400821685791,
                    # "cuda_malloc_retries": 0,
                    # "num_loss_counted_tokens": 4940, "batch_size": 14,
                    # "total_loss": 0.4069821238517761, "gradnorm": null,
                    # "weight_norm": 557.9681396484375,
                    # "timestamp": "2024-07-18T22:46:41.628932"}
                    if 'epoch' in d.keys():
                        dt = datetime.strptime(d['timestamp'], '%Y-%m-%dT%X.%f')
                        ts = math.floor(dt.timestamp() * 1000)
                        if first_ts == None:
                            first_ts = ts
                        sample = {'end': ts, 'value': d['overall_throughput']}
                        log_sample(file_id, desc, names, sample)
                        last_ts = ts
            metric_file_name = finish_samples()
            period['metric-files'].append(metric_file_name)
            iter_sample['periods'].append(period)

        # Now create the primary metric and the primary-period
        iter_sample['primary-metric'] = 'actual-train-seconds'
        period = { 'name': 'measurement', 'metric-files': [] }
        file_id = 'measurement'
        desc = {'source' : 'llm', 'class': 'count', 'type': 'actual-train-seconds'}
        names = {}
        sample = {'begin': first_ts, 'end': last_ts, 'value': (last_ts - first_ts) / 1000}
        log_sample(file_id, desc, names, sample)
        metric_file_name = finish_samples()
        period['metric-files'].append(metric_file_name)
        iter_sample['periods'].append(period)

    elif t_global.args.workflow == 'sdg':
        print('sdg')
        iter_sample['primary-metric'] = 'sdg-samples-sec'
        period = { 'name': 'measurement', 'metric-files': [] }
        file_id = 'measurement'
        desc = {'source' : 'llm', 'class': 'throughput', 'type': 'sdg-samples-sec'}
        names = {}

        iter_sample['primary-metric'] = 'sdg-samples-sec'
        num_skills = open("skills-num-samples.txt").readline().rstrip()
        print('skills num samples')
        print(num_skills)
        # INFO 2024-09-10 15:10:46,813 instructlab.sdg.datamixing:126: Dataset loaded with 70 samples
        # INFO 2024-09-09 20:39:58,535 instructlab.sdg:438: Generation took 1373.39s
        skipped_skills = False
        total_samples = int(0)
        # Add up the samples from all "dataset loaded" lines only after the skills are loaded
        # Skills are loaded by detecting the "dataset loaded" line with N samples,
        # where N = skills-num-samples.txt (which comes from counting lines in
        # /usr/share/instructlab/sdg/datasets/skills.jsonl during the benchmark
        with lzma.open("sdg-stderr.txt.xz", 'rt') as file:
            for line in file:
                if skipped_skills:
                    if reggy := re.search(r'^INFO\s(\d+-\d+-\d+\s\d+:\d+:\d+\d+,\d+).+Generation\stook\s(\d+\.\d+)s', line):
                        print('found generation line')
                        print(line)
                        elapsed_time = float(reggy.group(2))
                        end_dt = reggy.group(1)
                        samples_sec = float(total_samples / elapsed_time)
                        dt = datetime.strptime(end_dt, '%Y-%m-%d %X,%f')
                        end_ts = int(math.floor(dt.timestamp() * 1000))
                        begin_ts = int(end_ts - (elapsed_time * 1000))
                        # We have exactly 1 sample, so providing the 'begin' timestamp is required
                        sample = {'begin': begin_ts, 'end': end_ts, 'value': samples_sec}
                        print('log_sample')
                        log_sample(file_id, desc, names, sample)
                        break
                    if reggy := re.search(r'^INFO\s\d+-\d+-\d+\s\d+:\d+:\d+,\d+.+Dataset\sloaded\swith\s(\d+)\ssamples', line):
                        print('found Dataset line')
                        print(line)
                        total_samples = total_samples + int(reggy.group(1))
                else:
                    if reggy := re.search(r'^INFO\s\d+-\d+-\d+\s\d+:\d+:\d+,\d+\sinstructlab.sdg.datamixing:\d+:\sDataset\sloaded\swith\s(\d+)\ssamples', line):
                        print('found skills Dataset line')
                        print(line)
                        num_samples = reggy.group(1)
                        if num_samples == num_skills:
                            skipped_skills = True
        

        metric_file_name = finish_samples()
        period['metric-files'].append(metric_file_name)
        iter_sample['periods'].append(period)
    f = open('post-process-data.json', 'w')
    f.write(json.dumps(iter_sample))
    f.close
    return(0)


if __name__ == "__main__":
    exit(main())
