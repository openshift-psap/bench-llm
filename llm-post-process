#!/usr/bin/env python3
# -*- mode: python; indent-tabs-mode: nil; python-indent-level: 4 -*-
# vim: autoindent tabstop=4 shiftwidth=4 expandtab softtabstop=4 filetype=python

import sys
import json
from dataclasses import dataclass, is_dataclass, asdict
from enum import Enum
from typing import Dict, List, Any, Optional, Tuple
from functools import reduce

def to_dict(a: Any):
    if hasattr(a, "to_dict"):
        return a.to_dict()
    elif is_dataclass(a):
        return asdict(a)
    else:
        raise TypeError(f"Cannot bring {a} to a dict")

def lookup(d: Dict[Any, Any], path: str, delim: str=".") -> Optional[Any]:
    try:
        segments = path.split(delim)
        return reduce(lambda td, seg: td[seg], segments, d)
    except KeyError:
        return None


"""
The following enums and dataclasses are datastructures
to represent various pieces of how Crucible expects to
find data. These are not specific to bench-llm, but are
structures to help us model what the output of the
post-processing needs to look like.
"""

class MetricClass(Enum):
    COUNT = "count"
    THROUGHPUT = "throughput"

@dataclass
class Desc:
    metric_class: MetricClass
    source: str
    metric_type: str

    def to_dict(self):
        return {
                "class": str(self.metric_class.value),
                "source": self.source,
                "type": self.metric_type,
            }
@dataclass
class Metric:
    idx: int
    desc: Desc
    names: Dict[str, str]

    def to_dict(self):
        return {
                "desc": to_dict(self.desc),
                "idx": self.idx,
                "names": self.names,
            }

@dataclass
class Period:
    name: str
    metric_files: List[str]

    def to_dict(self):
        return {
                "name": self.name,
                "metric-files": self.metric_files,
            }

@dataclass
class PostProcess:
    primary_period: str
    primary_metric: str
    benchmark: str
    periods: List[Period]
    rickshaw_bench_metric_version: str

    def to_dict(self):
        return {
                "primary-period": self.primary_period,
                "primary-metric": self.primary_metric,
                "benchmark": self.benchmark,
                "periods": [to_dict(period) for period in self.periods],
                "rickshaw-bench-metric": {
                    "schema": {
                        "version": self.rickshaw_bench_metric_version
                    }
                }
            }

@dataclass
class Entry:
    idx: int
    start: int
    end: int
    value: float

    def __str__(self):
        return f"{int(self.idx)},{int(self.start)},{int(self.end)},{float(self.value)}"

"""
Helpers specific to guidellm and bench-llm
"""

@dataclass
class Phase():
    """
    An execution of a guidellm strategy. A rate-type of `sweep` will result in multiple
    runs, each using a different strategy. A constant/poisson/etc rate-type is a degenerate
    form of this sweep where there is only one run, using one strategy.
    There is a one-to-one correspondence between `Phase`'s and CDM `Period`'s. Each `Phase`
    will be given its own CDM `Period`.
    """
    name: str
    idx: int # "metric-data-[0-9]+"

def flatten_dict(d: Any, sep="_", prefix="") -> List[Tuple[str, float]]:
    pairs = []
    if type(d) is float:
        pairs.append((prefix, d))
    elif type(d) is dict:
        for k, v in d.items():
            pairs.extend(flatten_dict(v, sep=sep, prefix=(prefix + sep + k)))

    return pairs


def safe_name(name: str) -> str:
    return name.lower().replace("_", "-")

PROFILE_TYPE_MAP = {
    "sweep": 0.0,
    "async": 1.0,
}

STRATEGY_TYPE_MAP = {
    "synchronous": 0.0,
    "throughput": 1.0,
    "constant": 2.0,
}

def parse_phase(results: Dict[str, Any], phase: Phase, period: Period):
    benchmark = results["benchmarks"][phase.idx]
    args = benchmark["args"]
    data = benchmark["metrics"]
    run_stats = benchmark["run_stats"]

    json_path = period.metric_files[0] + ".json"
    csv_path = period.metric_files[0] + ".csv"

    # Guidellm gives floating point seconds, we need integer millis
    start_time = int(float(benchmark["start_time"])) * 1000
    end_time = int(float(benchmark["end_time"])) * 1000

    # All the metrics we are going to try to look for. Each of these might resolve to multiple
    # metrics with different suffixes.
    args_classes = [
        ("target_rps", MetricClass.THROUGHPUT, lambda x: x["strategy"]["rate"]),
        ("measured_rps", MetricClass.THROUGHPUT, lambda x: x["profile"]["measured_rates"][int(x["strategy_index"])]),
        ("measured_concurrency", MetricClass.COUNT, lambda x: x["profile"]["measured_concurrencies"][int(x["strategy_index"])]),
        ("profile_type", MetricClass.COUNT, lambda x: PROFILE_TYPE_MAP[x["profile"]["type_"]]),
        ("strategy_type", MetricClass.COUNT, lambda x: STRATEGY_TYPE_MAP[x["strategy"]["type_"]]),
    ]
    kpi_classes = [
        ("inter_token_latency_ms", MetricClass.COUNT),
        ("output_token_count", MetricClass.COUNT),
        ("output_tokens_per_second", MetricClass.THROUGHPUT),
        ("prompt_token_count", MetricClass.COUNT),
        ("request_concurrency", MetricClass.COUNT),
        ("request_latency", MetricClass.COUNT),
        ("requests_per_second", MetricClass.THROUGHPUT),
        ("time_per_output_token_ms", MetricClass.COUNT),
        ("time_to_first_token_ms", MetricClass.COUNT),
        ("tokens_per_second", MetricClass.THROUGHPUT),
    ]
    run_stats_classes = [
        ("worker_start_delay_avg", MetricClass.COUNT),
        ("worker_start_time_targeted_delay_avg", MetricClass.COUNT),
        ("worker_time_avg", MetricClass.COUNT),
        ("request_start_time_delay_avg", MetricClass.COUNT),
        ("request_start_time_targeted_delay_avg", MetricClass.COUNT),
        ("request_time_avg", MetricClass.COUNT),
        ("request_time_delay_avg", MetricClass.COUNT),
        ("requests_made", MetricClass.COUNT),
        ("scheduled_time_delay_avg", MetricClass.COUNT),
        ("scheduled_time_sleep_avg", MetricClass.COUNT),
        ("queued_time_avg", MetricClass.COUNT),
    ]

    # Build metrics list
    metrics: List[Metric] = []
    pairs = []
    idx_map = {}
    next_free_metric_idx = 0

    for arg in args_classes:
        try:
            idx_map[arg[0]] = next_free_metric_idx
            value = arg[2](args)
            if value is None:
                continue
            metrics.append(Metric(next_free_metric_idx, Desc(arg[1], "guidellm", safe_name(arg[0])), {}))
            pairs.append((arg[0], float(value)))
        except:
            continue
        finally:
            next_free_metric_idx += 1

    for kpi in kpi_classes:
        metric_values = flatten_dict(data[kpi[0]], prefix=kpi[0])
        for name, value in metric_values:
            if value is None:
                continue
            metrics.append(Metric(next_free_metric_idx, Desc(kpi[1], "guidellm", safe_name(name)), {}))
            idx_map[name] = next_free_metric_idx
            pairs.append((name, float(value)))
            next_free_metric_idx += 1

    for run_stat in run_stats_classes:
        metric_values = flatten_dict(run_stats[run_stat[0]], prefix=run_stat[0])
        for name, value in metric_values:
            if value is None:
                continue
            metrics.append(Metric(next_free_metric_idx, Desc(run_stat[1], "guidellm", safe_name(name)), {}))
            idx_map[name] = next_free_metric_idx
            pairs.append((name, float(value)))
            next_free_metric_idx += 1

    with open(json_path, "w") as f:
        f.write(json.dumps([to_dict(metric) for metric in metrics], indent=4))

    # Build entry list
    entries: List[Entry] = []
    for pair in pairs:
        entries.append(Entry(idx_map[pair[0]], start_time, end_time, pair[1]))

    with open(csv_path, "w") as f:
        f.write("\n".join([str(entry) for entry in entries]))

    return next_free_metric_idx

def main():

    # First, load the output json file. It will be required
    # to determine the structure of the payload as well as
    # actually containing the data
    output = {}
    with open("output.json", "r") as f:
        output = json.load(f)

    # In any benchmark post-process script, the metrics generated need to be attributed to a
    # time-period (AKA benchmark-phase).  The period which is used to report and offical
    # result for the benchmark is the 'measurement' period.  Other periods thay may exist
    # could be "warm-up", "prep", etc.

    # There are 3 important phases to the post-processing here...

    # 1. Generate the 'post-process-data.json` file which describes where to find
    #   our processed data and includes some metadata

    periods: List[Period] = [Period(f"p{i}", [f"metric-data-{i}"]) for i in range(len(output["benchmarks"]))]
    phases: List[Phase] = [Phase(f"p{i}", i) for i in range(len(output["benchmarks"]))]

    for phase, period in zip(phases, periods):
        parse_phase(output, phase, period)

    post_process_config = PostProcess(
        periods[0].name,
        safe_name("profile_type"),
        "llm",
        periods,
        '2021.04.12',
    )

    with open("post-process-data.json", "w") as f:
        f.write(json.dumps(to_dict(post_process_config), indent=4))


    return 0


if __name__ == "__main__":
    sys.exit(main())
